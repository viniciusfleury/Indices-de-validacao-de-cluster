{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"RN7a7OUSvuXw"},"source":["This is the CUDA parallel code for calculating the Dunn index, according to the definition of José María Luna-Romera (BD-Dunn)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1689016571447,"user":{"displayName":"Vinicius Fleury","userId":"17178576852718746748"},"user_tz":180},"id":"QCNfUNtW6Vsl"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34731,"status":"ok","timestamp":1689016606171,"user":{"displayName":"Vinicius Fleury","userId":"17178576852718746748"},"user_tz":180},"id":"0wwlziOT6clU","outputId":"2b7d6c3c-57f0-4e6d-86c9-e9cb4a0659f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":21325,"status":"ok","timestamp":1689016822673,"user":{"displayName":"Vinicius Fleury","userId":"17178576852718746748"},"user_tz":180},"id":"tWA0sTu43Wpq"},"outputs":[],"source":["da = pd.read_csv('/content/drive/MyDrive/Faculdade./TCC./VDU./Aplicando CUDA no VDU./Algoritmo LUNA./Data/luna_k5_f20_5000000.dat')"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":42452,"status":"ok","timestamp":1689016865683,"user":{"displayName":"Vinicius Fleury","userId":"17178576852718746748"},"user_tz":180},"id":"0I7KEsXA4RRi"},"outputs":[],"source":["nome_arquivo = 'luna_k5_f20_5000000.dat'\n","\n","# Salvando o DataFrame em um arquivo de texto\n","da.to_csv(nome_arquivo, sep=' ', index=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":7878,"status":"ok","timestamp":1689016873537,"user":{"displayName":"Vinicius Fleury","userId":"17178576852718746748"},"user_tz":180},"id":"6ciFRBnT9hqH"},"outputs":[],"source":["# Abrir o arquivo .dat em modo de leitura\n","with open('luna_k5_f20_5000000.dat', 'r') as arquivo:\n","    # Ler o conteúdo do arquivo\n","    conteudo = arquivo.read()\n","\n","    # Remover as aspas utilizando a função replace()\n","    conteudo_sem_aspas = conteudo.replace('\"', '')\n","\n","# Abrir o arquivo em modo de escrita e escrever o conteúdo sem as aspas\n","with open('luna_k5_f20_5000000.dat', 'w') as arquivo:\n","    arquivo.write(conteudo_sem_aspas)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":360,"status":"ok","timestamp":1689015872352,"user":{"displayName":"Vinicius Fleury","userId":"17178576852718746748"},"user_tz":180},"id":"2sXKjK30vr3v","outputId":"d3a3ca6b-ac56-416b-d501-ebe86703d400"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU op-mode(s):                  32-bit, 64-bit\n","CPU(s):                          2\n","On-line CPU(s) list:             0,1\n","CPU family:                      6\n","Model name:                      Intel(R) Xeon(R) CPU @ 2.00GHz\n","CPU MHz:                         2000.136\n","NUMA node0 CPU(s):               0,1\n","Using built-in specs.\n","COLLECT_GCC=g++\n","COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/9/lto-wrapper\n","OFFLOAD_TARGET_NAMES=nvptx-none:hsa\n","OFFLOAD_TARGET_DEFAULT=1\n","Target: x86_64-linux-gnu\n","Configured with: ../src/configure -v --with-pkgversion='Ubuntu 9.4.0-1ubuntu1~20.04.1' --with-bugurl=file:///usr/share/doc/gcc-9/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-9-Av3uEd/gcc-9-9.4.0/debian/tmp-nvptx/usr,hsa --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\n","Thread model: posix\n","gcc version 9.4.0 (Ubuntu 9.4.0-1ubuntu1~20.04.1) \n"]}],"source":["# Check the CPU available and the gcc version\n","!lscpu | grep CPU\n","!g++ -v"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":516,"status":"ok","timestamp":1689015874175,"user":{"displayName":"Vinicius Fleury","userId":"17178576852718746748"},"user_tz":180},"id":"mxE5QtfR8Jl0","outputId":"6ad7d61d-95c5-4f81-dd36-ec8608336125"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Jul 10 19:04:34 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Wed_Sep_21_10:33:58_PDT_2022\n","Cuda compilation tools, release 11.8, V11.8.89\n","Build cuda_11.8.r11.8/compiler.31833905_0\n"]}],"source":["#Check the GPU available and the nvcc version\n","!nvidia-smi\n","!nvcc --version"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6225,"status":"ok","timestamp":1689015881655,"user":{"displayName":"Vinicius Fleury","userId":"17178576852718746748"},"user_tz":180},"id":"plBFsjBj9JFN","outputId":"7b957486-f165-40c8-9d23-5dd0396b26cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n","  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-17uexs_g\n","  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-17uexs_g\n","  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: NVCCPlugin\n","  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4287 sha256=db55c1ae250a1453173c0562b4bd1768f5276d605bc8ad063d7b600a153db32c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-xelxu8a7/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n","Successfully built NVCCPlugin\n","Installing collected packages: NVCCPlugin\n","Successfully installed NVCCPlugin-0.0.2\n","created output directory at /content/src\n","Out bin /content/result.out\n"]}],"source":["# Install nvcc4jupter to easily work with CUDA file and install the nvcc plugin\n","!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n","%load_ext nvcc_plugin"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29814,"status":"ok","timestamp":1689016941667,"user":{"displayName":"Vinicius Fleury","userId":"17178576852718746748"},"user_tz":180},"id":"UrnFONTyPw9A","outputId":"15f282db-083a-443d-d418-68e39b83b8f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Centroid Global: 0.75 0.55 0.35 0.65 0.45 0.55 0.45 0.55 0.65 0.45 0.55 0.65 0.45 0.45 0.65 0.45 0.65 0.45 0.45 0.55 \n","Min intercluster 0.96\n","Max intracluster 0.41\n","The Dunn index: 2.36\n","Time taken: 408.757001 milissegundos\n","\n"]}],"source":["%%cu\n","// This is the CUDA program for Dunn index calculation\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <float.h>\n","#include <math.h>\n","\n","//luna_k5_f20_5000 -- MP:5120, BS:128, MB:MP/BS=40\n","//luna_k5_20_10000 -- MP:16384, BS:128, MB:MP/BS=128\n","//luna_k5_20_25000 -- MP:32768, BS:128, MB:MP/BS=256\n","//luna_k5_20_50000 -- MP:65536, BS:128, MB:MP/BS=512\n","//luna_k5_f20_500000 -- MP:524288, BS:128, MB:MP/BS=4096\n","//luna_k5_20_2500000 -- MP:4194304, BS:128, MB:MP/BS=32768\n","//luna_k5_20_5000000 -- MP:8388608, BS:128, MB:MP/BS=65536\n","// Test_k2_f2_10, Iris_k3_f4_150 - Digits_k10_f64_1797 - Electricity_k2_f8_45311\n","// MAX_POINTS 2048 (others) or 65536 (Electric)\n","// NF 2 (Test) 4 (Iris) 64 (Digits) 8 (Electricity)\n","// BLOCK_SIZE 128 (others) 64 (Digits)\n","#define MAX_POINTS 8388608\n","#define NF 20\n","#define BLOCK_SIZE 128\n","\n","#define MAX_CLUSTERS 20\n","#define MAX_BLOCKS 65536\n","\n","// Calculate the euclidian distance between two points in the CPU\n","float distance(float v1[], float v2[]) {\n","    float sum = 0.0;\n","    for (int d = 0; d < NF; d++) {\n","        sum += pow((v1[d] - v2[d]), 2);\n","    }\n","    return sqrt(sum);\n","}\n","\n","// Calculate the euclidian distance between two points in the GPU\n","__device__ float distance(float *s_point, int p, int q, int nfeat) {\n","    float sum = 0.0;\n","    for (int d = 0; d < nfeat; d++) {\n","        sum = sum + pow((s_point[p+d] - s_point[q+d]), 2);\n","    }\n","    return sqrt(sum);\n","}\n","\n","// Kernel that calculates the parcial sums (in each dimenstion) of the instances coordenates\n","__global__ void centroids(int cluster, float *d_centroid_tmp, float *d_point, int *d_cluster_start, int nfeat) {\n","\n","  __shared__ float s_centroid[BLOCK_SIZE * NF];\n","  int tid = threadIdx.x;\n","  int lower = d_cluster_start[cluster];\n","  int i = (blockIdx.x * blockDim.x) + threadIdx.x;\n","  int size = d_cluster_start[cluster+1] - d_cluster_start[cluster];\n","  int p; //reduction step: 64, 32, 16, 8, 4, 2,1\n","\n","  // All threads initialize with zero the shared memory\n","  for (int d = 0; d < nfeat; d++) {\n","    s_centroid[tid * nfeat + d] = 0.0;\n","  }\n","  __syncthreads();\n","\n","  // Copy points from global memory to shared memory\n","  if (i < size) {\n","    for (int d = 0; d < nfeat; d++) {\n","      s_centroid[tid * nfeat + d] = (float ) d_point[(lower + i)*nfeat + d];\n","    }\n","  }\n","  __syncthreads();\n","\n","  // Perform a local reduction on the memory shared data\n","  // It starts with 64 threads, then 32, 16, 8, 4, 2, 1\n","  p = blockDim.x / 2;\n","  while (p != 0) {\n","    if (tid < p) {\n","\t    for (int d = 0; d < nfeat; d++) {\n","        s_centroid[tid*nfeat+d] = s_centroid[tid*nfeat+d] + s_centroid[(tid+p)*nfeat+d];\n","      }\n","    }\n","    __syncthreads();\n","    p = p/2;\n","  }\n","\n","  // Thread zero of each block moves the local result to the global memory\n","  if (tid == 0) {\n","    for (int d = 0; d < nfeat; d++) {\n","        d_centroid_tmp[blockIdx.x * nfeat + d] = (float )s_centroid[d];\n","    }\n","  }\n","}\n","\n","// Kernel that finds the cluster/point with the greatest distance from the centroid\n","__global__ void maxs_intra(int cluster, int *d_index_tmp, float *d_maxs_tmp, float *d_centroid, float *d_point, int *d_cluster_start, int nfeat) {\n","\n","  __shared__ float s_point[(BLOCK_SIZE+1)*NF];\n","  __shared__ int s_pos[(BLOCK_SIZE+1)];\n","  int tid = threadIdx.x;\n","  int lower = d_cluster_start[cluster];\n","  int i = (blockIdx.x * blockDim.x) + threadIdx.x;\n","  int size = d_cluster_start[cluster+1] - d_cluster_start[cluster];\n","  int nb, d, p, r, q;\n","  float dist;\n","\n","  // All threads initialize shared memory\n","  s_pos[tid] = lower + tid;\n","  for (d = 0; d < nfeat; d++) {\n","    s_point[tid*nfeat+d] = 0.0;\n","  }\n","  __syncthreads();\n","\n","  // Copy data points from global memory to shared memory\n","  if (i < size) {\n","    for (d = 0; d < nfeat; d++) {\n","      s_point[tid*nfeat+d] = d_point[(lower+i)*nfeat+d];\n","    }\n","  }\n","\n","  // store centroid in the last position of the vector shared memory to save memory\n","  if (tid == 0) {\n","    for (d = 0; d < nfeat; d++) {\n","      s_point[blockDim.x*nfeat+d] = d_centroid[cluster*nfeat+d];\n","    }\n","  }\n","  __syncthreads();\n","\n","  // adjust limit for the last block\n","  if (blockIdx.x == (gridDim.x -1)) {\n","    nb = size % blockDim.x;\n","  } else {\n","    nb = blockDim.x;\n","  }\n","\n","  // each thread calculates dist\n","  if (tid < nb) {\n","    r = tid*nfeat; // point index\n","    q = blockDim.x*nfeat; // centroid index\n","    dist = distance(s_point, r, q, nfeat);\n","    s_point[tid*nfeat] = dist;\n","  }\n","  __syncthreads();\n","\n","  // reduction to find the maximum distance\n","  p = blockDim.x / 2; // log steps\n","  while (p != 0) {\n","    if (tid < p) {\n","      if (s_point[tid*nfeat] < s_point[(tid+p)*nfeat]) {\n","        s_point[tid*nfeat] = s_point[(tid+p)*nfeat];\n","        s_pos[tid] = s_pos[tid+p];\n","      }\n","    }\n","    __syncthreads();\n","    p = p/2;\n","  }\n","\n","  // Thread zero of each block copy data to glocal memory\n","  if (tid == 0) {\n","    d_index_tmp[blockIdx.x] = s_pos[0];\n","    d_maxs_tmp[blockIdx.x] = s_point[0];\n","  }\n","}\n","\n","int main()\n","{\n","  int num_clusters; // number of clusters\n","  int cluster_size[MAX_CLUSTERS]; //cluster sizes\n","  static float point[MAX_POINTS][NF]; // cluster data\n","  float *d_point; // GPU cluster data\n","  float centroid[MAX_CLUSTERS][NF]; // centroid data\n","  float *d_centroid; // GPU centroid data\n","  float centroid_tmp[MAX_BLOCKS][NF]; // centroid temporary data\n","  float centroid_g[NF];\n","  float *d_centroid_tmp; // GPU centroid temporary data\n","  int index_tmp[MAX_BLOCKS]; // index temporary data\n","  int *d_index_tmp; // GPU index temporary data\n","  float maxs_tmp[MAX_BLOCKS]; // max values temporary\n","  float *d_maxs_tmp; // GPU max values temporary\n","  int cluster_start[MAX_CLUSTERS+1]; // start cluster indexes\n","  int *d_cluster_start; // GPU start cluster indexes\n","  FILE *fp; // file pointer\n","  int size = 0; // total number of points\n","  int nfeat; // number of attributes\n","  clock_t start, stop; // measure time\n","  float running_time; // running time\n","  int nblocks; // number of blocks\n","  int cluster; // current cluster\n","  float sum; // sum of elements\n","  float dist; // distance\n","  float max_distance; // maximum distance\n","  float min_distance; // minimum distance\n","  int cluster1; // cluster chosen\n","  int p1; // index chosen\n","\n","  // Input the number of clusters and the cluster information\n","  // Format: 1st line: #clusters #features, 2nd: cluster sizes, 3rd: data\n","\n","\n","\n","  //fp = fopen(\"luna_k5_f20_5000.dat\", \"r\");\n","  //fp = fopen(\"luna_k5_f20_10000.dat\", \"r\");\n","  //fp = fopen(\"luna_k5_f20_25000.dat\", \"r\");\n","  //fp = fopen(\"luna_k5_f20_50000.dat\", \"r\");\n","  //fp = fopen(\"luna_k5_f20_500000.dat\", \"r\");\n","  //fp = fopen(\"luna_k5_f20_2500000.dat\", \"r\");\n","  fp = fopen(\"luna_k5_f20_5000000.dat\", \"r\");\n","  //fp = fopen(\"test_k2_f2_10.dat\", \"r\");\n","  //fp = fopen(\"iris_k3_f4_150.dat\", \"r\");\n","  //fp = fopen(\"digits_k10_f64_1797.dat\", \"r\");\n","  //fp = fopen(\"electricity_k2_f8_45311.dat\", \"r\");\n","\n","\n","  // Read file (upload file first if running in Collab)\n","  fscanf(fp, \"%d %d\", &num_clusters, &nfeat);\n","  for (int k = 0; k < num_clusters; k++) {\n","    fscanf(fp, \"%d\", &cluster_size[k]);\n","    size = size + cluster_size[k];\n","  }\n","  for (int i = 0; i < size; i++) {\n","    for (int j = 0; j < nfeat; j++) {\n","       fscanf(fp, \"%f\", &point[i][j]);\n","\n","       }\n","  }\n","  fclose(fp);\n","\n","  // prefix sum to find out the beginning of each cluster\n","  cluster_start[0] = 0;\n","  for (int i = 1; i < num_clusters+1; i++) {\n","    cluster_start[i] = cluster_start[i-1] + cluster_size[i-1];\n","  }\n","\n","   // initialize global centroid with zero\n","  for (int j = 0; j < nfeat; j++) {\n","    centroid_g[j] = 0.0;\n","  }\n","\n","  // Allocate GPU memory\n","  cudaMalloc(&d_cluster_start, (MAX_CLUSTERS+1)*sizeof(int));\n","  cudaMalloc(&d_point, MAX_POINTS*NF*sizeof(float));\n","  cudaMalloc(&d_centroid_tmp, MAX_BLOCKS*NF*sizeof(float));\n","  cudaMalloc(&d_index_tmp, MAX_BLOCKS*sizeof(int));\n","  cudaMalloc(&d_maxs_tmp, MAX_BLOCKS*sizeof(float));\n","  cudaMalloc(&d_centroid, MAX_CLUSTERS*NF*sizeof(float));\n","\n","  // start clock to measure running time\n","  start = clock();\n","\n","  // Copy data (cluster points and start indices) to the GPU\n","  cudaMemcpy(d_point, point, MAX_POINTS*nfeat*sizeof(float), cudaMemcpyHostToDevice);\n","  cudaMemcpy(d_cluster_start, cluster_start, (MAX_CLUSTERS+1)*sizeof(int), cudaMemcpyHostToDevice);\n","\n","  //tam = 0;\n","  // find centroids: launch the kernel for each cluster\n","  for (cluster = 0; cluster < num_clusters; cluster++) {\n","\n","    // Number of blocks is size of cluster divided by the block size\n","    nblocks = (cluster_size[cluster] + BLOCK_SIZE - 1) / BLOCK_SIZE;\n","\n","\n","    // launch kernel and verify if got any error\n","    centroids<<<nblocks, BLOCK_SIZE>>>( cluster, d_centroid_tmp, d_point, d_cluster_start, nfeat );\n","    cudaError_t error = cudaGetLastError();\n","    if(error != cudaSuccess) { printf(\"CUDA error: %s\\n\", cudaGetErrorString(error)); exit(-1); }\n","\n","    // Wait for the kernel to finish and copy centroid temporary data for the host (CPU)\n","    // The kernel returns the parcial sums of each block\n","    cudaDeviceSynchronize();\n","    cudaMemcpy(&centroid_tmp, d_centroid_tmp, MAX_BLOCKS*NF*sizeof(float), cudaMemcpyDeviceToHost);\n","    cudaDeviceSynchronize();\n","\n","\n","    // Calculate centroid and store it in the centroid_tmp\n","    // The parcial sums need to be accumulated and divided by the cluster size\n","    for (int i = 0; i < nfeat; i++) {\n","      sum = 0.0;\n","      for (int j = 0; j < nblocks; j++) {\n","        sum = sum + centroid_tmp[j][i];\n","      }\n","      centroid_g[i] = centroid_g[i] + sum;\n","      centroid_tmp[0][i] = sum / (float )cluster_size[cluster];\n","      centroid[cluster][i] = centroid_tmp[0][i];\n","    }\n","    //tam=tam+(float )cluster_size[cluster];\n","  }\n","\n","\n","  printf(\"\\nCentroid Global: \");\n","  for (int i = 0; i < nfeat; i++) {\n","     centroid_g[i] = centroid_g[i] / size;\n","     printf(\"%.2f \", centroid_g[i]);\n","  }\n","\n","  // Copy centroids to the GPU\n","  cudaMemcpy(d_centroid, centroid, MAX_CLUSTERS*NF*sizeof(float), cudaMemcpyHostToDevice);\n","\n","  // Find the global centroid and store at centroid[MAX_CLUSTERS][NF]\n","  // Initialize global centroid with zero\n","  for (int k = 0; k < nfeat; k++) {\n","    centroid[MAX_CLUSTERS][k] = 0.0;\n","  }\n","  for (int k = 0; k < nfeat; k++) {\n","    for (int i = 0; i < num_clusters; i++) {\n","      centroid[MAX_CLUSTERS][k] = centroid[MAX_CLUSTERS][k] + centroid[i][k];\n","    }\n","  }\n","  for (int k = 0; k < nfeat; k++) {\n","    centroid[MAX_CLUSTERS][k] = centroid[MAX_CLUSTERS][k] / num_clusters;\n","  }\n","\n","  // Find the centroid closer to the global centroid\n","  min_distance = DBL_MAX;\n","  for (int i = 0; i < num_clusters; i++) {\n","    for (int k = 0; k < nfeat; k++) {\n","      dist = distance(centroid[i], centroid_g);\n","      if (dist < min_distance) {\n","        min_distance = dist;\n","      }\n","    }\n","  }\n","  // Now min_distance is the numerator of the Dunn index\n","\n","  // Now, find maximum diameter launching the kernel for each cluster again\n","  max_distance = 0;\n","  for (cluster = 0; cluster < num_clusters; cluster++) {\n","\n","    // Number of blocks is size of cluster divided by the block size\n","    nblocks = (cluster_size[cluster] + BLOCK_SIZE - 1) / BLOCK_SIZE;\n","\n","    // launch kernel and verify if got any error\n","    maxs_intra<<<nblocks, BLOCK_SIZE>>>( cluster, d_index_tmp, d_maxs_tmp, d_centroid, d_point, d_cluster_start, nfeat );\n","    cudaError_t error = cudaGetLastError();\n","    if(error != cudaSuccess) { printf(\"CUDA error: %s\\n\", cudaGetErrorString(error)); exit(-1); }\n","\n","    // Wait for the kernel to finish and copy maximum temporary data for the host (CPU)\n","    // The kernel returns the several maximums, one for each block\n","    cudaDeviceSynchronize();\n","    cudaMemcpy(&maxs_tmp, d_maxs_tmp, MAX_BLOCKS*sizeof(float), cudaMemcpyDeviceToHost);\n","    cudaMemcpy(&index_tmp, d_index_tmp, MAX_BLOCKS*sizeof(int), cudaMemcpyDeviceToHost);\n","    cudaDeviceSynchronize();\n","\n","    // Calculate the global maximum and store it in the max_distance\n","    // The parcial maximums need to be compared and the global maximum stored\n","    // The cluster and the maximum point position need to be saved (cluster1 and p1)\n","    for (int j = 0; j < nblocks; j++) {\n","      if (maxs_tmp[j] > max_distance) {\n","        max_distance = maxs_tmp[j];\n","        p1 = index_tmp[j];\n","        cluster1 = cluster;\n","      }\n","    }\n","  }\n","\n","  // finalize runtime calculation\n","  stop = clock();\n","\n","  // Print results\n","  printf(\"\\nMin intercluster %.2f\", min_distance);\n","  printf(\"\\nMax intracluster %.2f\", max_distance);\n","  printf(\"\\nThe Dunn index: %.2f\", min_distance / max_distance);\n","\n","  // Print the time taken\n","  running_time = (float)(stop - start) / CLOCKS_PER_SEC;\n","  printf(\"\\nTime taken: %lf milissegundos\\n\", 1000.0*running_time);\n","\n","  // Free GPU memory\n","  cudaFree( d_cluster_start );\n","  cudaFree( d_point );\n","  cudaFree( d_centroid_tmp );\n","  cudaFree( d_index_tmp );\n","  cudaFree( d_maxs_tmp );\n","  cudaFree( d_centroid );\n","\n","  return 0;\n","}"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["fdIL2OGpMwTO","wUAbmODJMy_i"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
